{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39af9430",
   "metadata": {},
   "source": [
    "# Lab 1: Hugging Face Transformers — Encoder vs Decoder\n",
    "\n",
    "Goal: Learn to use the `transformers` library to work with models, both encoder-only (e.g., BERT) and decoder-only (e.g., Gemma 1B).\n",
    "\n",
    "In this lab we will cover:\n",
    "- Overview of the library and model types\n",
    "- Import and use of BERT, focusing on `cache_dir` and loading/inference parameters\n",
    "- Use of a Gemma model, focusing on special tokens, tokenization, and chat template\n",
    "- Two hands-on exercises: embedding similarity and how to construct effective prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad882c",
   "metadata": {},
   "source": [
    "## Step 0 - What is Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ddde2",
   "metadata": {},
   "source": [
    "<style>\n",
    "img[src=\"images/image4.png\"] {\n",
    "    width: 40%;\n",
    "    height: auto;\n",
    "}\n",
    "</style>\n",
    "![alt text](images/image4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e690d5d3",
   "metadata": {},
   "source": [
    "## Lab overview: encoder-only vs decoder-only\n",
    "\n",
    "- Encoder-only models (e.g., BERT): produce text representations (embeddings). Ideal for classification, semantic search, and similarity.\n",
    "- Decoder-only models (e.g., Gemma): generate text autoregressively and are suited for dialogue, completion, and extraction via prompting.\n",
    "\n",
    "The `transformers` library offers uniform interfaces for tokenizers and models, supports caching with `cache_dir`, and includes utilities such as a chat template for conversation-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cf857d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7ffb397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data01/pferrazzi/miniconda3/envs/med_instr/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.52.3\n"
     ]
    }
   ],
   "source": [
    "# Setup: import and config\n",
    "import os, json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "print('Transformers version:', __import__('transformers').__version__)\n",
    "\n",
    "# Main paths (local cache for models)\n",
    "BASE_DIR = os.path.join('lab1')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "CACHE_DIR = os.path.join(BASE_DIR, 'models_cache')\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e09834b",
   "metadata": {},
   "source": [
    "We will use `bert-base-uncased` to generate sentence embeddings. Focus:\n",
    "- `cache_dir`: local directory to save weights/tokenizer\n",
    "- Loading parameters: dtype, device map, etc. (here, CPU-only)\n",
    "- Inference: obtain embeddings from `last_hidden_state` and compute cosine similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aafe745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:\n",
      " BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "The model has been loaded on memory using this data type: torch.float32\n",
      "The model has been loaded on this device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Caricamento tokenizer e modello BERT\n",
    "bert_model_id = 'bert-base-uncased'\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(bert_model_id, cache_dir=CACHE_DIR)\n",
    "model_bert = AutoModel.from_pretrained(bert_model_id, cache_dir=CACHE_DIR)\n",
    "\n",
    "print('Model structure:\\n', model_bert, \"\\n\\n\")\n",
    "print('The model has been loaded on memory using this data type:', next(model_bert.parameters()).dtype)\n",
    "print('The model has been loaded on this device:', next(model_bert.parameters()).device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e4b3ed",
   "metadata": {},
   "source": [
    "Example on how to embed a sequence using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9246f4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  {'input_ids': tensor([[7592, 1010, 2129, 2024, 2017, 1029]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "Embeddings shape: torch.Size([1, 6, 768]) \n",
      "\n",
      "Embeddings: tensor([[[-0.3992, -0.0344, -0.3397,  ..., -0.4190, -0.0928,  1.1807],\n",
      "         [-0.1067,  0.1923,  0.3294,  ..., -0.3010,  0.6376,  0.7872],\n",
      "         [-0.0539,  0.2979,  0.4493,  ..., -0.0907,  0.5909,  0.7460],\n",
      "         [-0.1186, -0.0581,  0.4523,  ..., -0.1695,  0.7302,  0.5671],\n",
      "         [-0.2087, -0.2150,  0.3977,  ...,  0.0065,  0.7225,  0.6818],\n",
      "         [ 0.0020, -0.0546,  0.3174,  ..., -0.2015,  0.6058,  0.6682]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example on how to embed a sequence using BERT\n",
    "text = \"Hello, how are you?\"\n",
    "inputs = tokenizer_bert(text, return_tensors='pt', add_special_tokens=False)\n",
    "\n",
    "print('input: ', inputs)\n",
    "example_input_ids = inputs['input_ids']\n",
    "example_attention_mask = inputs['attention_mask']\n",
    "\n",
    "outputs = model_bert(example_input_ids, attention_mask=example_attention_mask)\n",
    "embeddings = outputs.last_hidden_state\n",
    "print('\\nEmbeddings shape:', embeddings.shape, '\\n')\n",
    "print('Embeddings:', embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad924711",
   "metadata": {},
   "source": [
    "The same can be done providing a list of values as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac018445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings of a list shape: torch.Size([2, 8, 768]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs_of_a_list = tokenizer_bert([text]*2, return_tensors='pt')\n",
    "outputs_of_a_list = model_bert(**inputs_of_a_list)\n",
    "embeddings_of_a_list = outputs_of_a_list.last_hidden_state\n",
    "print('Embeddings of a list shape:', embeddings_of_a_list.shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782996e0",
   "metadata": {},
   "source": [
    "So far we got a list of embeddings, one per token in the sentence. How to select one embedding to represent this sentence?\n",
    "- Extract the vector representation for a specific position,\n",
    "- Average among all embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4eedf2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  {'input_ids': tensor([[ 2023,  2003,  1037,  6251,  2057,  2052,  2066,  2000, 26268,  1012]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "Embeddings shape: torch.Size([1, 768]) \n",
      "\n",
      "Embeddings (first 10 el): tensor([ 0.6958,  0.1477, -0.0319,  0.1394,  0.3625, -0.0942, -0.0405, -0.3588,\n",
      "         0.0238, -0.4620], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Embeddings shape: torch.Size([1, 768]) \n",
      "\n",
      "Embeddings (first 10 el): tensor([ 0.5057,  0.0430, -0.1341,  0.3907,  0.2379, -0.1453, -0.0711, -0.1567,\n",
      "         0.4575, -0.6209], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a sentence we would like to classify.\"\n",
    "inputs = tokenizer_bert(text, return_tensors='pt', add_special_tokens=False)\n",
    "\n",
    "print('input: ', inputs)\n",
    "example_input_ids = inputs['input_ids']\n",
    "example_attention_mask = inputs['attention_mask']\n",
    "\n",
    "outputs = model_bert(example_input_ids, attention_mask=example_attention_mask)\n",
    "\n",
    "# Extract the vector representation for a specific position,\n",
    "pos_token_on_interest = 3\n",
    "embeddings_one_pos = outputs.last_hidden_state[:, pos_token_on_interest, :]\n",
    "print('\\nEmbeddings shape:', embeddings_one_pos.shape, '\\n')\n",
    "print('Embeddings (first 10 el):', embeddings_one_pos[0][:10])\n",
    "# Average pooling of the token embeddings\n",
    "embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "print('\\nEmbeddings shape:', embeddings.shape, '\\n')\n",
    "print('Embeddings (first 10 el):', embeddings[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a3c42",
   "metadata": {},
   "source": [
    "Text embeddings are commonly used to define `similarity` between text sequences. They act as vectorial representations of the text itself.\n",
    "\n",
    "The similarity between two sequences can then be calculated as the cosine similarity:\n",
    "<style>\n",
    "img[src=\"images/image.png\"],\n",
    "img[src=\"images/image2.png\"] {\n",
    "    width: 20%;\n",
    "    height: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "![alt text](images/image.png) \n",
    "![alt text](images/image2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fd1d80",
   "metadata": {},
   "source": [
    "## Gemma: special tokens, tokenization, chat template, prompting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed990135",
   "metadata": {},
   "source": [
    "Preliminary note: to use some models from HuggingFace, you often have to accept their terms of use. For this lab, it is not required, but you are highly encouraged to signup to HuggingFace \n",
    "\n",
    "<style>\n",
    "img[src=\"images/image3.png\"] {\n",
    "    width: 30%;\n",
    "    height: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "![alt text](images/image3.png)\n",
    "\n",
    "When you work inside a script/notebook, you can let the environment know that it is you by using your access token ([here](https://huggingface.co/docs/hub/security-tokens) more info), by doing so\n",
    "\n",
    "```python\n",
    "from huggingface_hub import login\n",
    "login(YOUR_ACCESS_TOKEN)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38f6c1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.52.3\n"
     ]
    }
   ],
   "source": [
    "# Setup: import and config\n",
    "import os, json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "print('Transformers version:', __import__('transformers').__version__)\n",
    "\n",
    "# Main paths (local cache for models)\n",
    "BASE_DIR = os.path.join('../../lab1')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "CACHE_DIR = os.path.join(BASE_DIR, 'models_cache')\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53edad38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma model: Gemma3ForCausalLM(\n",
      "  (model): Gemma3TextModel(\n",
      "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-17): 18 x Gemma3DecoderLayer(\n",
      "        (self_attn): Gemma3Attention(\n",
      "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
      "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
      "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Gemma3MLP(\n",
      "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
      "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
      "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "    (rotary_emb): Gemma3RotaryEmbedding()\n",
      "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
      ") \n",
      "\n",
      "\n",
      "Special tokens: {'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'boi_token': '<start_of_image>', 'eoi_token': '<end_of_image>', 'image_token': '<image_soft_token>'} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path= 'google/gemma-3-270m-it' # 'unsloth/gemma-3-1B-it' # USED TO AVOID HAVING TO ACCEPT TERMS OF SERVICE FOR THE GEMMA MODEL\n",
    "device = 'cuda'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=CACHE_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    cache_dir=\"/data01/pferrazzi/.cache\", # \"CACHE_DIR, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=device\n",
    "    )\n",
    "\n",
    "print('Gemma model:', model, '\\n\\n')\n",
    "print('Special tokens:', tokenizer.special_tokens_map, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "471923f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: {'input_ids': tensor([[  3689,    659,    506,   1791,   2432,    531,    776,    528, 194486,\n",
      "         236881]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')} \n",
      "\n",
      "\n",
      "Generated token ids: tensor([[  3689,    659,    506,   1791,   2432,    531,    776,    528, 194486,\n",
      "         236881,  21950,  63582,  65785, 125378,  18628, 202891, 202891, 202891,\n",
      "         202891, 202891]], device='cuda:0') \n",
      "\n",
      "\n",
      "Generated text: ['What are the best things to do in Padova?？” moyens”?الجبالключенияключенияключенияключенияключения'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic inference\n",
    "\n",
    "prompt = 'What are the best things to do in Padova?'\n",
    "\n",
    "# first, you need to tokenize the prompt \n",
    "tokenized_prompt = tokenizer(\n",
    "    prompt, \n",
    "    return_tensors='pt',\n",
    "    add_special_tokens=False\n",
    ").to(device)\n",
    "print('Tokenized prompt:', tokenized_prompt, '\\n\\n')\n",
    "\n",
    "# then, you can generate a response from the model\n",
    "generated_ids = model.generate(\n",
    "    **tokenized_prompt,\n",
    "    max_new_tokens=10,\n",
    "    temperature=0.001\n",
    ")\n",
    "print('Generated token ids:', generated_ids, '\\n\\n')\n",
    "\n",
    "# now we need to use the tokenizer to map back the generated token ids to text\n",
    "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)\n",
    "print('Generated text:', generated_text, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404dfdc5",
   "metadata": {},
   "source": [
    "This does not work properly because models have been trained with specific special tokens, that need to be inserted in the right positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ea92c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'What are the best things to do in Padova?'\n",
    "\n",
    "# let's turn add_special_tokens to True\n",
    "tokenized_prompt = tokenizer(\n",
    "    prompt, \n",
    "    return_tensors='pt',\n",
    "    add_special_tokens=True,\n",
    ").to(device)\n",
    "\n",
    "# then, you can generate a response from the model\n",
    "generated_ids = model.generate(\n",
    "    tokenized_prompt['input_ids'],\n",
    "    max_new_tokens=10,\n",
    "    temperature=0.001\n",
    ")\n",
    "print('Generated token ids:', generated_ids, '\\n\\n')\n",
    "\n",
    "# now we need to use the tokenizer to map back the generated token ids to text\n",
    "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)\n",
    "print('Generated text:', generated_text, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a8339",
   "metadata": {},
   "source": [
    "Even more, instruction-tuned models have undergone a training phase with a specific chat-like structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6d4404",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What are the best things to do in Padova?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5502f",
   "metadata": {},
   "source": [
    "### Parametri di generazione utili\n",
    "- `max_new_tokens`: massimo numero di token generati\n",
    "- `temperature`: 0.0 per output più deterministici\n",
    "- `top_p`/`top_k`: campionamento nucleare\n",
    "- `repetition_penalty`: penalizza ripetizioni\n",
    "\n",
    "Nota: l'esecuzione decoder-only è più fluida con GPU. In CPU, limitare `max_new_tokens` e usare temperature basse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d714f0",
   "metadata": {},
   "source": [
    "### Excercise 1\n",
    "You are provided a list of sentences at `lab1/data/sentences.txt`. Your need to:\n",
    "- find the pair of sentences with the highest similarity\n",
    "- find the sentence that has the highest overall similarity to the others. The overall similarity of a string *_s_* can be calculated as the sum of the similarities of *_s_* and all other strings.\n",
    "\n",
    "**Hints**\n",
    "function to calculate the norm of a vector: `np.linalg.norm()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6865743",
   "metadata": {},
   "source": [
    "### Excercise 2\n",
    "\n",
    "You are provided 3 sentences at `lab1/data/target_text.txt`. You need to construct an effective prompt to extract the mentions of people from the text. You can use the examples provided in `lab1/data/few_shot_examples.json`. You are expected to try different prompt setups, included elaborated system prompts and few-shot example prompting. \\\\\n",
    "You need to perform inference :\n",
    "- just providing the target text and a brief description of the task\n",
    "- providing a proper system prompt\n",
    "- providing an example on how the task needs to be performed in the user prompt\n",
    "- providing an example of how the task needs to be performed as multi-turn conversation\n",
    "- providing multiple examples of how the task needs to be performed as multi-turn conversation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_instr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
