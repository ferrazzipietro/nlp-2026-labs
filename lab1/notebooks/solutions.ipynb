{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f4c9b68",
   "metadata": {},
   "source": [
    "### Excercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7214bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: import and config\n",
    "import os, json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# Main paths (local cache for models)\n",
    "BASE_DIR = os.path.join('../../lab1')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "CACHE_DIR = os.path.join(BASE_DIR, 'models_cache')\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "bert_model_id = 'bert-base-uncased'\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(bert_model_id, cache_dir=CACHE_DIR)\n",
    "model_bert = AutoModel.from_pretrained(bert_model_id, cache_dir=CACHE_DIR)\n",
    "\n",
    "\n",
    "# Excercise 1: embedding and similarity among 5 sentences\n",
    "sentences_path = os.path.join(DATA_DIR, 'sentences.txt')\n",
    "# if not os.path.exists(sentences_path):\n",
    "#     # Notebook runs from lab1/notebooks; fallback to relative path\n",
    "#     sentences_path = os.path.join('..', '..', 'data', 'sentences.txt')\n",
    "\n",
    "with open(sentences_path, 'r', encoding='utf-8') as f:\n",
    "    sentences = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Tokenize\n",
    "enc = tokenizer_bert(sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Inference (embedding as the mean of the last_hidden_state along the token dimension)\n",
    "with torch.no_grad():\n",
    "    outputs = model_bert(**enc)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()  # shape: (N, hidden)\n",
    "\n",
    "eps = 1e-12\n",
    "norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "norms = np.maximum(norms, eps)\n",
    "sim_matrix = (embeddings @ embeddings.T) / (norms @ norms.T)\n",
    "\n",
    "\n",
    "# exclude self-similarity for argmax\n",
    "np.fill_diagonal(sim_matrix, -np.inf)\n",
    "\n",
    "best_i, best_j = np.unravel_index(np.argmax(sim_matrix), sim_matrix.shape)\n",
    "print('Most similar pair — indices:', (best_i, best_j))\n",
    "print('Sentence A:', sentences[best_i])\n",
    "print('Sentence B:', sentences[best_j])\n",
    "print('Cosine similarity:', float(sim_matrix[best_i, best_j]))\n",
    "\n",
    "# overall similarity (exclude diagonal)\n",
    "sim_no_diag = sim_matrix.copy()\n",
    "np.fill_diagonal(sim_no_diag, 0.0)\n",
    "overall_sim = sim_no_diag.sum(axis=1)\n",
    "best_overall_idx = np.argmax(overall_sim)\n",
    "print('\\nMost similar overall — index:', best_overall_idx, 'overall similarity:', float(overall_sim[best_overall_idx]))\n",
    "print('Sentence:', sentences[best_overall_idx])\n",
    "print('Similarities to others:', sim_no_diag[best_overall_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3283be87",
   "metadata": {},
   "source": [
    "### Excercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb811197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: import and config\n",
    "import os, json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "print('Transformers version:', __import__('transformers').__version__)\n",
    "\n",
    "# Main paths (local cache for models)\n",
    "BASE_DIR = os.path.join('../../lab1')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "CACHE_DIR = os.path.join(BASE_DIR, 'models_cache')\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "model_path= 'unsloth/gemma-3-1B-it' # 'google/gemma-3-270m-it' #  USED TO AVOID HAVING TO ACCEPT TERMS OF SERVICE FOR THE GEMMA MODEL\n",
    "device = 'cuda'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=CACHE_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    cache_dir=\"/data01/pferrazzi/.cache\", # \"CACHE_DIR, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=device\n",
    "    )\n",
    "\n",
    "print('Gemma model:', model, '\\n\\n')\n",
    "print('Special tokens:', tokenizer.special_tokens_map, '\\n\\n')\n",
    "\n",
    "target_text_path = os.path.join(DATA_DIR, 'target_text.txt')\n",
    "with open(target_text_path, 'r', encoding='utf-8') as f:\n",
    "    target_text_list = f.readlines()\n",
    "    target_text_list = [t.strip() for t in target_text_list]\n",
    "\n",
    "\n",
    "def apply_chat_template_and_tokenize(messages_list: list, device=None):\n",
    "    input = [tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        for messages in messages_list\n",
    "    ]\n",
    "    tokenized = tokenizer(input, return_tensors='pt', padding=True)# .to(device)\n",
    "    return tokenized\n",
    "    \n",
    "###############  \n",
    "# 0-shot prompt\n",
    "###############\n",
    "\n",
    "messages_zero_shot_list = [\n",
    "    [\n",
    "        { 'role': 'user', 'content': f'Find the Persons in the following text:\\n{target_text}'}\n",
    "    ] for target_text in target_text_list\n",
    "]\n",
    "tokenized_0 = apply_chat_template_and_tokenize(messages_zero_shot_list)\n",
    "outputs_0 = model.generate(**tokenized_0, max_new_tokens=40)\n",
    "print('0-shot output:', tokenizer.decode(outputs_0[0][tokenized_0[\"input_ids\"].shape[-1]:]), '\\n\\n')\n",
    "\n",
    "\n",
    "###############  \n",
    "# with system prompt\n",
    "###############\n",
    "\n",
    "system_prompt = (\n",
    "    'You are an assistant that extracts PERSON names from the given text. '\n",
    "    'Respond with a JSON array of strings containing the person names found. '\n",
    "    'If none are present, respond with an empty array.'\n",
    ")\n",
    "messages_zero_shot_list = [\n",
    "    [\n",
    "        { 'role': 'system', 'content': system_prompt },\n",
    "        { 'role': 'user', 'content': f'{target_text}'}\n",
    "    ] for target_text in target_text_list\n",
    "]\n",
    "tokenized_0 = apply_chat_template_and_tokenize(messages_zero_shot_list)\n",
    "outputs_0 = model.generate(**tokenized_0, max_new_tokens=40)\n",
    "print('0-shot output:', tokenizer.decode(outputs_0[0][tokenized_0[\"input_ids\"].shape[-1]:]), '\\n\\n')\n",
    "\n",
    "###############  \n",
    "# with system prompt and example provided as user message\n",
    "###############\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'few_shot_examples.json'), 'r', encoding='utf-8') as f:\n",
    "    examples = json.load(f)\n",
    "\n",
    "example_text = examples[0]['input']\n",
    "example_answer = examples[0]['output']\n",
    "\n",
    "messages_one_shot_list = [\n",
    "    [\n",
    "        { 'role': 'system', 'content': system_prompt },\n",
    "        { 'role': 'user', 'content': example_text + example_answer },\n",
    "        { 'role': 'user', 'content': f'{target_text}'}\n",
    "    ] for target_text in target_text_list\n",
    "]\n",
    "tokenized_1 = apply_chat_template_and_tokenize(messages_one_shot_list)\n",
    "outputs_1 = model.generate(**tokenized_1, max_new_tokens=40)\n",
    "print('1-shot output:', tokenizer.decode(outputs_1[0][tokenized_1[\"input_ids\"].shape[-1]:]), '\\n\\n')\n",
    "\n",
    "\n",
    "###############  \n",
    "# with system prompt and 1 example provided as assistant message\n",
    "###############\n",
    "\n",
    "messages_one_shot_list = [\n",
    "    [\n",
    "        { 'role': 'system', 'content': system_prompt },\n",
    "        { 'role': 'user', 'content': example_text },\n",
    "        { 'role': 'assistant', 'content': example_answer },\n",
    "        { 'role': 'user', 'content': f'{target_text}'}\n",
    "    ] for target_text in target_text_list\n",
    "]\n",
    "tokenized_1 = apply_chat_template_and_tokenize(messages_one_shot_list)\n",
    "outputs_1 = model.generate(**tokenized_1, max_new_tokens=40)\n",
    "print('1-shot output:', tokenizer.decode(outputs_1[0][tokenized_1[\"input_ids\"].shape[-1]:]), '\\n\\n')\n",
    "\n",
    "###############  \n",
    "# with system prompt and all examples provided as assistant message\n",
    "###############\n",
    "\n",
    "messages_system_and_few_shot= [\n",
    "    { 'role': 'system', 'content': system_prompt }\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    messages_system_and_few_shot.append({ 'role': 'user', 'content': example['input'] })\n",
    "    messages_system_and_few_shot.append({ 'role': 'assistant', 'content': example['output'] })\n",
    "\n",
    "messages_few_shot_list = [\n",
    "    [\n",
    "        *messages_system_and_few_shot,\n",
    "        { 'role': 'user', 'content': f'{target_text}'}\n",
    "    ] for target_text in target_text_list\n",
    "]\n",
    "tokenized_few_shot = apply_chat_template_and_tokenize(messages_few_shot_list)\n",
    "outputs_few_shot = model.generate(**tokenized_few_shot, max_new_tokens=40)\n",
    "print('few-shot output:', tokenizer.decode(outputs_few_shot[0][tokenized_few_shot[\"input_ids\"].shape[-1]:]), '\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
