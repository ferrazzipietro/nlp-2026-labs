{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29aaee49",
   "metadata": {},
   "source": [
    "# Lab 2: Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Objective: Build a minimal RAG pipeline using `transformers` with an encoder-only embedder (BERT) and a decoder-only generator (Llama 1B). We focus on understanding internal mechanisms, not efficiency.\n",
    "\n",
    "This lab includes:\n",
    "- A brief RAG overview\n",
    "- Indexing Padua documents with BERT embeddings (token-based chunking)\n",
    "- Retrieving top-n similar chunks by cosine similarity\n",
    "- Generating short answers with Llama using retrieved context\n",
    "- Exercises matching course goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d29e91",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "We reuse patterns from Lab 1: local `cache_dir`, simple pooling for embeddings, and an optional Llama generation flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f1796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "print('Transformers version:', __import__('transformers').__version__)\n",
    "print('Torch version:', torch.__version__)\n",
    "\n",
    "BASE_DIR = os.path.join('lab2')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "CACHE_DIR = os.path.join(BASE_DIR, 'models_cache')\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "RUN_LLAMA = False  # toggle to True to actually generate answers\n",
    "\n",
    "# Model IDs (adjust if needed)\n",
    "BERT_ID = 'bert-base-uncased'\n",
    "LLAMA_PRIMARY = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "LLAMA_FALLBACK = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63f2da2",
   "metadata": {},
   "source": [
    "## Load Embedder (BERT) and Tokenizer\n",
    "We use BERT as an encoder-only embedder. Embeddings are computed via mean pooling of the last hidden states across tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a91690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bert = AutoTokenizer.from_pretrained(BERT_ID, cache_dir=CACHE_DIR)\n",
    "model_bert = AutoModel.from_pretrained(BERT_ID, cache_dir=CACHE_DIR)\n",
    "print('BERT special tokens:', tokenizer_bert.special_tokens_map)\n",
    "print('BERT dtype:', next(model_bert.parameters()).dtype)\n",
    "print('BERT device:', next(model_bert.parameters()).device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c061c6",
   "metadata": {},
   "source": [
    "## Load Generator (Llama 1B)\n",
    "We attempt to load a 1B chat model. If the primary model is unavailable, we fall back to TinyLlama 1.1B. Generation is optional and requires resources (GPU recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d95ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llama(model_id):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, cache_dir=CACHE_DIR)\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, cache_dir=CACHE_DIR, torch_dtype=torch.float16, device_map='auto'\n",
    "    )\n",
    "    return tok, mdl\n",
    "\n",
    "try:\n",
    "    tokenizer_llama, model_llama = load_llama(LLAMA_PRIMARY)\n",
    "    LLAMA_ID = LLAMA_PRIMARY\n",
    "except Exception as e:\n",
    "    print('Fallback to TinyLlama due to:', type(e).__name__, str(e)[:300])\n",
    "    tokenizer_llama, model_llama = load_llama(LLAMA_FALLBACK)\n",
    "    LLAMA_ID = LLAMA_FALLBACK\n",
    "\n",
    "print('Llama model:', LLAMA_ID)\n",
    "print('Has chat template?', bool(getattr(tokenizer_llama, 'chat_template', None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e0d5e",
   "metadata": {},
   "source": [
    "## Load Documents and Prepare Chunking\n",
    "We index 25 Padua documents. We split each document into token-based chunks using the BERT tokenizer (e.g., 128 tokens per chunk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4784c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_path = os.path.join(DATA_DIR, 'kb_docs.json')\n",
    "with open(docs_path, 'r', encoding='utf-8') as f:\n",
    "    DOCS = json.load(f)\n",
    "len(DOCS), DOCS[0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5efc7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking utility: split text into chunks capped by max_tokens\n",
    "def chunk_text(text, tokenizer, max_tokens=128):\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=False, add_special_tokens=False)\n",
    "    input_ids = tokens['input_ids'][0].tolist()\n",
    "    chunks = []\n",
    "    for i in range(0, len(input_ids), max_tokens):\n",
    "        sub_ids = input_ids[i:i+max_tokens]\n",
    "        chunk_txt = tokenizer.decode(sub_ids)\n",
    "        chunks.append({'text': chunk_txt, 'token_count': len(sub_ids)})\n",
    "    return chunks\n",
    "\n",
    "# Build KB chunks\n",
    "KB_CHUNKS = []\n",
    "for d in DOCS:\n",
    "    chs = chunk_text(d['text'], tokenizer_bert, max_tokens=128)\n",
    "    for idx, ch in enumerate(chs):\n",
    "        KB_CHUNKS.append({\n",
    "            'doc_id': d['id'],\n",
    "            'chunk_id': idx,\n",
    "            'title': d['title'],\n",
    "            'text': ch['text'],\n",
    "            'token_count': ch['token_count']\n",
    "        })\n",
    "len(KB_CHUNKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b570a4e",
   "metadata": {},
   "source": [
    "## Compute Embeddings and Save KB Index\n",
    "We compute mean-pooled BERT embeddings for each chunk and save a JSON KB index containing text, embedding, and embedding_dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc92ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts(texts, tokenizer, model, batch_size=8):\n",
    "    embs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(batch, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            out = model(**enc)\n",
    "            pooled = out.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "            embs.extend(pooled.tolist())\n",
    "    return np.array(embs)\n",
    "\n",
    "chunk_texts = [c['text'] for c in KB_CHUNKS]\n",
    "EMB = embed_texts(chunk_texts, tokenizer_bert, model_bert, batch_size=8)\n",
    "EMB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save KB index\n",
    "kb_index = []\n",
    "for i, c in enumerate(KB_CHUNKS):\n",
    "    emb = EMB[i].tolist()\n",
    "    kb_index.append({\n",
    "        'doc_id': c['doc_id'],\n",
    "        'chunk_id': c['chunk_id'],\n",
    "        'title': c['title'],\n",
    "        'text': c['text'],\n",
    "        'embedding': emb,\n",
    "        'embedding_dim': len(emb),\n",
    "        'token_count': c['token_count']\n",
    "    })\n",
    "index_path = os.path.join(DATA_DIR, 'kb_index.json')\n",
    "with open(index_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(kb_index, f, ensure_ascii=False, indent=2)\n",
    "len(kb_index), kb_index[0]['embedding_dim']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7670a21c",
   "metadata": {},
   "source": [
    "## Retrieval: Top-n Similar Chunks\n",
    "We compute cosine similarity between the query embedding and KB chunk embeddings and return the top-n chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4bb7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_matrix(A, B):\n",
    "    A = A / np.clip(np.linalg.norm(A, axis=1, keepdims=True), 1e-12, None)\n",
    "    B = B / np.clip(np.linalg.norm(B, axis=1, keepdims=True), 1e-12, None)\n",
    "    return A @ B.T\n",
    "\n",
    "def embed_query(query):\n",
    "    enc = tokenizer_bert([query], return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        out = model_bert(**enc)\n",
    "        emb = out.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return emb\n",
    "\n",
    "def retrieve_top_n(query, kb_emb, kb_meta, n=3):\n",
    "    q = embed_query(query)\n",
    "    sims = cosine_similarity_matrix(q, kb_emb)[0]\n",
    "    top_idx = np.argsort(-sims)[:n]\n",
    "    return [(float(sims[i]), kb_meta[i]) for i in top_idx]\n",
    "\n",
    "# Load queries\n",
    "queries_path = os.path.join(DATA_DIR, 'queries.json')\n",
    "with open(queries_path, 'r', encoding='utf-8') as f:\n",
    "    QUERIES = json.load(f)\n",
    "\n",
    "# Demo retrieval on first query\n",
    "q0 = QUERIES[0]['query']\n",
    "top_chunks = retrieve_top_n(q0, EMB, kb_index, n=3)\n",
    "for sim, meta in top_chunks:\n",
    "    print(f\n",
    ", 'title=', meta['title'], '\n",
    "text=', meta['text'][:180], '\n",
    "')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d6895",
   "metadata": {},
   "source": [
    "## Generation: Answer from Retrieved Context\n",
    "We instruct Llama to answer concisely using only the provided context. If the answer is not in the context, the model should say it does not know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c5203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(chunks):\n",
    "    ctx = ''\n",
    "    for _, meta in chunks:\n",
    "        ctx += f"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
