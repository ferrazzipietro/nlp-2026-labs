{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29aaee49",
   "metadata": {},
   "source": [
    "# Lab 2: Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Objective: Build a minimal RAG pipeline using `transformers` with an encoder-only embedder (BERT) and a decoder-only generator (gemma 1B). We focus on understanding internal mechanisms, not efficiency.\n",
    "\n",
    "This lab includes:\n",
    "- A brief RAG overview\n",
    "- Indexing Padua documents with BERT embeddings (token-based chunking)\n",
    "- Retrieving top-n similar chunks by cosine similarity\n",
    "- Generating short answers with gemma using retrieved context\n",
    "- Exercises matching course goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984cacbd",
   "metadata": {},
   "source": [
    "<style>\n",
    "img[src=\"images/image.png\"] {\n",
    "    width: 30%,\n",
    "    height: 20%;\n",
    "}\n",
    "</style>\n",
    "\n",
    "![alt text](image.png)\n",
    "\n",
    "_img from https://drjulija.github.io/posts/basic-rag/_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d29e91",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "We reuse patterns from Lab 1: local `cache_dir`, simple pooling for embeddings, and an optional gemma generation flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f9f1796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.52.3\n",
      "Torch version: 2.7.0+cu126\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "print('Transformers version:', __import__('transformers').__version__)\n",
    "print('Torch version:', torch.__version__)\n",
    "\n",
    "BASE_DIR = os.path.join('../../lab2')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "CACHE_DIR = os.path.join(BASE_DIR, 'models_cache')\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "RUN_gemma = False  # toggle to True to actually generate answers\n",
    "\n",
    "# Model IDs (adjust if needed)\n",
    "BERT_ID = 'bert-base-uncased'\n",
    "GEMMA_ID = 'unsloth/gemma-3-1B-it'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63f2da2",
   "metadata": {},
   "source": [
    "## Load Embedder (BERT) and Tokenizer\n",
    "We use BERT as an encoder-only embedder. Embeddings are computed via mean pooling of the last hidden states across tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a91690b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "BERT dtype: torch.float32\n",
      "BERT device: cpu\n"
     ]
    }
   ],
   "source": [
    "tokenizer_bert = AutoTokenizer.from_pretrained(BERT_ID, cache_dir=CACHE_DIR)\n",
    "model_bert = AutoModel.from_pretrained(BERT_ID, cache_dir=CACHE_DIR)\n",
    "print('BERT special tokens:', tokenizer_bert.special_tokens_map)\n",
    "print('BERT dtype:', next(model_bert.parameters()).dtype)\n",
    "print('BERT device:', next(model_bert.parameters()).device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c061c6",
   "metadata": {},
   "source": [
    "## Load Generator (GEMMA)\n",
    "We attempt to load a 1B chat model. If the primary model is unavailable, we fall back to Tinygemma 1.1B. Generation is optional and requires resources (GPU recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6d95ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma model: unsloth/gemma-3-1B-it\n",
      "Has chat template? True\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_id):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, cache_dir=CACHE_DIR)\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, cache_dir=CACHE_DIR, torch_dtype=torch.float16, device_map='auto'\n",
    "    )\n",
    "    return tok, mdl\n",
    "\n",
    "tokenizer_gemma, model_gemma = load_model(GEMMA_ID)\n",
    "\n",
    "print('gemma model:', GEMMA_ID)\n",
    "print('Has chat template?', bool(getattr(tokenizer_gemma, 'chat_template', None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e0d5e",
   "metadata": {},
   "source": [
    "## Load Documents, Embed them and save to Knowledge Base\n",
    "We index 25 Padua documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4784c7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 25 docs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Prato della Valle'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_path = os.path.join(DATA_DIR, 'kb_docs.json')\n",
    "with open(docs_path, 'r', encoding='utf-8') as f:\n",
    "    DOCS = json.load(f)\n",
    "print('there are', len(DOCS), 'docs')\n",
    "DOCS[0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5efc7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 'Prato della Valle')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build KB chunks\n",
    "KB_CHUNKS = []\n",
    "for d in DOCS:\n",
    "    text_to_encode = d['text']\n",
    "    encodeing_input = tokenizer_bert(text_to_encode, return_tensors='pt', truncation=False, add_special_tokens=False)\n",
    "    hidden_states = model_bert(**encodeing_input).last_hidden_state\n",
    "    embedding = hidden_states.mean(dim=1).squeeze().detach().cpu().numpy()\n",
    "    KB_CHUNKS.append({\n",
    "        'doc_id': d['id'],\n",
    "        'title': d['title'],\n",
    "        'text': text_to_encode,\n",
    "        'token_count': len(text_to_encode.split()),\n",
    "        'embedding': embedding.tolist()\n",
    "    })\n",
    "\n",
    "index_path = os.path.join(DATA_DIR, 'kb_index.json')\n",
    "with open(index_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(KB_CHUNKS, f, ensure_ascii=False, indent=2)\n",
    "len(KB_CHUNKS), KB_CHUNKS[0]['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7670a21c",
   "metadata": {},
   "source": [
    "## Retrieval: Top-n Similar Chunks\n",
    "We compute cosine similarity between the query embedding and KB chunk embeddings and return the top-n chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef4bb7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(query):\n",
    "    enc = tokenizer_bert([query], return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        out = model_bert(**enc)\n",
    "        emb = out.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return emb.tolist()[0]\n",
    "\n",
    "def cosine_similarity_matrix(query, kb_emb):\n",
    "    query = np.array(query).reshape(1, -1)\n",
    "    kb_emb = np.array(kb_emb)\n",
    "    query = query / np.clip(np.linalg.norm(query, axis=1, keepdims=True), 1e-12, None)\n",
    "    kb_emb = kb_emb / np.clip(np.linalg.norm(kb_emb, axis=1, keepdims=True), 1e-12, None)\n",
    "    return query @ kb_emb.T\n",
    "\n",
    "def retrieve_top_n(query, kb_emb, n=3):\n",
    "    q = embed_query(query)\n",
    "    document_embeddings = [chunk['embedding'] for chunk in kb_emb]\n",
    "    sims = cosine_similarity_matrix(q, document_embeddings)[0]\n",
    "    top_idx = np.argsort(-sims)[:n]\n",
    "    return [{\n",
    "        'text': kb_emb[i]['text'], \n",
    "        'similarity': float(sims[i])} \n",
    "        for i in top_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ceb5a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Where can you see Giotto’s frescoes in Padua?\n",
      "Similarity=0.621, text=Founded in 1222, the University of Padua is renowned for research excellence, diverse faculties, and a vibrant international community.\n",
      "Similarity=0.618, text=The Scrovegni Chapel contains Giotto’s famous frescoes. Entry is by ticket and timed slots, located near the Church of the Eremitani.\n",
      "Similarity=0.598, text=Padua Cathedral (Duomo) stands alongside a Romanesque Baptistery with frescoes by Giusto de’ Menabuoi. Regular liturgical celebrations take place.\n"
     ]
    }
   ],
   "source": [
    "# Load queries\n",
    "queries_path = os.path.join(DATA_DIR, 'queries.json')\n",
    "with open(queries_path, 'r', encoding='utf-8') as f:\n",
    "    QUERIES = json.load(f)\n",
    "\n",
    "\n",
    "# Demo retrieval on first query\n",
    "q0 = QUERIES[0]['query']\n",
    "print('Query:', q0)\n",
    "top_chunks = retrieve_top_n(q0, KB_CHUNKS, n=3)\n",
    "for retrieved in top_chunks:\n",
    "    print(f'Similarity={retrieved[\"similarity\"]:.3f}, text={retrieved[\"text\"][:180]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d6895",
   "metadata": {},
   "source": [
    "## Generation: Answer from Retrieved Context\n",
    "We instruct GEMMA to answer concisely using only the provided context. If the answer is not in the context, the model should say it does not know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c5203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a helpful assistant that answers questions about the city of Padova, Italy. \n",
    "Use the retrieved documents to answer the question as best as you can. \n",
    "If you don't know the answer, say you don't know.\"\"\"\n",
    "\n",
    "\n",
    "RESULTS = []\n",
    "for query in QUERIES:\n",
    "    top_chunks = retrieve_top_n(query['query'], KB_CHUNKS, n=3)\n",
    "    for retrieved in top_chunks:\n",
    "        print(f'Similarity={retrieved[\"similarity\"]:.3f}, text={retrieved[\"text\"][:180]}')\n",
    "        \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": query['query']},\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer_gemma.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model_gemma.device)\n",
    "\n",
    "    outputs = model_gemma.generate(**inputs, max_new_tokens=40)\n",
    "    answer = tokenizer_gemma.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:])\n",
    "    RESULTS.append({\n",
    "        'query': query['query'],\n",
    "        'retrieved': top_chunks,\n",
    "        'answer': answer\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6db113",
   "metadata": {},
   "source": [
    "## Excercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc9c10a",
   "metadata": {},
   "source": [
    "1) You have generated an answer for each of the queries in `lab2/data/queries.json`. Each query has an `expected_answer`. Find a way to use Gemma to determine whether the generated answer is correct or not by prompting the LLM itself.\n",
    "\n",
    "2) Create a RAG system that answer the queries in `lab2/data/excercise/queries.json` using the documents in `lab2/data/excercise/docs.txt`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_instr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
