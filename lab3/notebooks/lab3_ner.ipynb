{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbb0af94",
   "metadata": {},
   "source": [
    "# Lab 3: Named Entity Recognition (NER) with Transformers\n",
    "\n",
    "Goals:\n",
    "1) Use a BERT-based token classification model for NER.\n",
    "2) Prompt a Gemma chat model to perform NER.\n",
    "3) Evaluate results for both approaches.\n",
    "\n",
    "We'll use a small English dataset with PERSON/ORG/LOC entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07155768",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We adopt the same caching pattern as previous labs. Gemma generation is optional (enable only with sufficient resources)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "413c5a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers: 4.52.3\n",
      "Torch: 2.7.0+cu126\n"
     ]
    }
   ],
   "source": [
    "import os, json, re\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForCausalLM, pipeline\n",
    "\n",
    "BASE_DIR = os.path.join('../../lab3')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "CACHE_DIR = os.path.join(BASE_DIR, 'models_cache')\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "RUN_GEMMA = False\n",
    "\n",
    "print('Transformers:', __import__('transformers').__version__)\n",
    "print('Torch:', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ae3e42",
   "metadata": {},
   "source": [
    "## Data: sentences with gold entities\n",
    "We evaluate on a small dataset with PERSON/ORG/LOC labels defined in `lab3/data/ner_examples.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0bf8b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 'Barack Obama visited Stanford University in California.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(DATA_DIR, 'ner_examples.json'), 'r', encoding='utf-8') as f:\n",
    "    DATA = json.load(f)\n",
    "len(DATA), DATA[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e3e9e0",
   "metadata": {},
   "source": [
    "## Part 1: BERT-based NER (token classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8628387c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForTokenClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
      ")\n",
      "Trained to assign these labels:  {0: 'O', 1: 'B-MISC', 2: 'I-MISC', 3: 'B-PER', 4: 'I-PER', 5: 'B-ORG', 6: 'I-ORG', 7: 'B-LOC', 8: 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "BERT_NER_ID = 'dslim/bert-base-NER'\n",
    "# Load tokenizer and token classification model directly (no pipeline)\n",
    "tokenizer_bert_ner = AutoTokenizer.from_pretrained(BERT_NER_ID, cache_dir=CACHE_DIR)\n",
    "model_bert_ner = AutoModelForTokenClassification.from_pretrained(BERT_NER_ID, cache_dir=CACHE_DIR)\n",
    "print(model_bert_ner)\n",
    "id2label = model_bert_ner.config.id2label\n",
    "print('Trained to assign these labels: ', id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82ddcb28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 14319, 7661, 3891, 8036, 1239, 1107, 1756, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 6), (7, 12), (13, 20), (21, 29), (30, 40), (41, 43), (44, 54), (0, 0)]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, identify the offset of each token in the original text\n",
    "text = 'Barack Obama visited Stanford University in California'\n",
    "enc_offsets = tokenizer_bert_ner(text, return_offsets_mapping=True, truncation=True)\n",
    "offsets = enc_offsets['offset_mapping']\n",
    "enc_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d40e174a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 9])\n",
      "Logits for one token:  tensor([ 8.4647, -0.5055, -1.1914, -0.5454, -1.4987, -1.2059, -1.9625, -1.5004,\n",
      "        -1.2675], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# use the tokenizer and model to get the logits for the input text\n",
    "# assign a probability to each token to belong to one of the classes\n",
    "inputs = tokenizer_bert_ner(input, return_tensors='pt')\n",
    "logits = model_bert_ner(**inputs).logits\n",
    "print(logits.shape)\n",
    "print('Logits for one token: ', logits[0][0])\n",
    "\n",
    "# find the actual predictions based on the probabilities\n",
    "pred_ids = logits.argmax(dim=-1)[0].tolist()\n",
    "token_ids = inputs['input_ids'][0].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef0febc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def assign_labels_to_tokens(pred_ids, token_ids, text, verbose=False):\n",
    "    entities = []\n",
    "    current_label = None\n",
    "    current_start = None\n",
    "    current_end = None\n",
    "\n",
    "\n",
    "    for i, (start, end) in enumerate(offsets):\n",
    "        tok = tokenizer_bert_ner.convert_ids_to_tokens([token_ids[i]])[0]\n",
    "        if tok == '[CLS]':\n",
    "            continue\n",
    "        if verbose: print(f\"Token: {tok}\")\n",
    "        # Skip special tokens or tokens without character span\n",
    "        if (start == 0 and end == 0):\n",
    "            if verbose: print(f\"  not an entity\")\n",
    "            pred_lbl = 'O'\n",
    "        else:\n",
    "            raw_lbl = id2label[pred_ids[i]]\n",
    "            pred_lbl = 'O' if raw_lbl == 'O' else raw_lbl\n",
    "            if verbose: print(f\"  found label: {raw_lbl}\")\n",
    "        if pred_lbl != 'O':\n",
    "            if current_label == pred_lbl and current_end == start:\n",
    "                # extend current span\n",
    "                current_end = end\n",
    "                if verbose: print(f\" this token is part of an already found entity!\")\n",
    "            else:\n",
    "                # close any previous span\n",
    "                if current_label is not None:\n",
    "                    span_text = text[current_start:current_end]\n",
    "                    entities.append({'text': span_text, 'label': current_label})\n",
    "                    if verbose: print(f\"  end of the entity\")\n",
    "                # start new span\n",
    "                current_label = pred_lbl\n",
    "                current_start = start\n",
    "                current_end = end\n",
    "        else:\n",
    "            if current_label is not None:\n",
    "                span_text = text[current_start:current_end]\n",
    "                entities.append({'text': span_text, 'label': current_label})\n",
    "                current_label = None\n",
    "                current_start = None\n",
    "                current_end = None\n",
    "    # close tail span if any\n",
    "    if current_label is not None:\n",
    "        span_text = text[current_start:current_end]\n",
    "        entities.append({'text': span_text, 'label': current_label})\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed5c502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Barack\n",
      "  found label: B-PER\n",
      "Token: Obama\n",
      "  found label: I-PER\n",
      "  end of the entity\n",
      "Token: visited\n",
      "  found label: O\n",
      "Token: Stanford\n",
      "  found label: B-ORG\n",
      "Token: University\n",
      "  found label: I-ORG\n",
      "  end of the entity\n",
      "Token: in\n",
      "  found label: O\n",
      "Token: California\n",
      "  found label: B-LOC\n",
      "Token: .\n",
      "  not an entity\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'Barack', 'label': 'B-PER'},\n",
       " {'text': 'Obama', 'label': 'I-PER'},\n",
       " {'text': 'Stanford', 'label': 'B-ORG'},\n",
       " {'text': 'University', 'label': 'I-ORG'},\n",
       " {'text': 'California', 'label': 'B-LOC'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "entities = assign_labels_to_tokens(pred_ids, token_ids, text, verbose=True)\n",
    "display(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c33f6c",
   "metadata": {},
   "source": [
    "## Part 2: Gemma prompting for NER\n",
    "We prompt a chat model to extract entities and return JSON with keys `PERSON`, `ORG`, `LOC`.\n",
    "We demonstrate zero-shot and few-shot (3-shot) prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8da9dd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat model: unsloth/gemma-3-1B-it\n",
      "Has chat template? True\n",
      "Zero-shot preview: <bos><start_of_turn>user\n",
      "You extract named entities from text. Return a JSON object with keys PERSON, ORG, LOC, each mapped to an array of strings. Use exact surface forms from the text and avoid duplicates.\n",
      "\n",
      "Text:Barack Obama visited Stanford University in California.Return only JSON.<end_of_turn>\n",
      "\n",
      "Three-shot preview: <bos><start_of_turn>user\n",
      "You extract named entities from text. Return a JSON object with keys PERSON, ORG, LOC, each mapped to an array of strings. Use exact surface forms from the text and avoid duplicates.\n",
      "\n",
      "Barack Obama spoke at Stanford University in California.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "{\"PERSON\": [\"Barack Obama\"], \"ORG\": [\"Stanford University\"], \"LOC\": [\"California\"]}<end_of_turn>\n",
      "<st\n"
     ]
    }
   ],
   "source": [
    "GEMMA_ID = 'unsloth/gemma-3-1B-it'\n",
    "\n",
    "def load_chat_model(model_id):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, cache_dir=CACHE_DIR)\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(model_id, cache_dir=CACHE_DIR, torch_dtype=torch.float16, device_map='auto')\n",
    "    return tok, mdl\n",
    "\n",
    "tokenizer_chat, model_chat = load_chat_model(GEMMA_ID)\n",
    "\n",
    "print('Chat model:', GEMMA_ID)\n",
    "print('Has chat template?', bool(getattr(tokenizer_chat, 'chat_template', None)))\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    'You extract named entities from text. Return a JSON object with keys PERSON, ORG, LOC, each mapped to an array of strings. Use exact surface forms from the text and avoid duplicates.'\n",
    ")\n",
    "\n",
    "def build_messages_zero_shot(text):\n",
    "    return [\n",
    "        {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "        {'role': 'user', 'content': 'Text:' + text + 'Return only JSON.'}\n",
    "    ]\n",
    "\n",
    "# Few-shot examples\n",
    "with open(os.path.join(DATA_DIR, 'few_shot_ner_examples.json'), 'r', encoding='utf-8') as f:\n",
    "    FEW = json.load(f)\n",
    "\n",
    "def build_messages_three_shot(text):\n",
    "    msgs = [{'role': 'system', 'content': SYSTEM_PROMPT}]\n",
    "    for ex in FEW:\n",
    "        msgs.append({'role': 'user', 'content': ex['input']})\n",
    "        msgs.append({'role': 'assistant', 'content': json.dumps(ex['output'])})\n",
    "    msgs.append({'role': 'user', 'content': text})\n",
    "    return msgs\n",
    "\n",
    "def generate_json_entities(text, few_shot=False):\n",
    "    messages = build_messages_three_shot(text) if few_shot else build_messages_zero_shot(text)\n",
    "    if getattr(tokenizer_chat, 'apply_chat_template', None):\n",
    "        input_ids = tokenizer_chat.apply_chat_template(messages, return_tensors='pt').to(model_chat.device)\n",
    "    else:\n",
    "        prompt = SYSTEM_PROMPT + 'Text:' + text + 'Return only JSON.'\n",
    "        input_ids = tokenizer_chat(prompt, return_tensors='pt').input_ids.to(model_chat.device)\n",
    "    if RUN_GEMMA:\n",
    "        gen = model_chat.generate(input_ids, max_new_tokens=128, temperature=0.0)\n",
    "        out = tokenizer_chat.decode(gen[0], skip_special_tokens=True)\n",
    "        return out\n",
    "    else:\n",
    "        return tokenizer_chat.decode(input_ids[0])\n",
    "\n",
    "# Preview prompts (no generation)\n",
    "print('Zero-shot preview:', generate_json_entities(DATA[0]['text'], few_shot=False)[:400])\n",
    "print('Three-shot preview:', generate_json_entities(DATA[0]['text'], few_shot=True)[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e367a9",
   "metadata": {},
   "source": [
    "## Part 3: Evaluation\n",
    "We evaluate entity-level precision/recall/F1 by exact text match per label.\n",
    "For the BERT pipeline, we map model labels to `PERSON/ORG/LOC`. For the chat model, we parse its JSON output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e176b759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT micro (avg): {'precision': 0.875, 'recall': 0.9166666666666666, 'f1': 0.8923809523809524}\n",
      "Chat zero-shot micro (avg): {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
      "Chat three-shot micro (avg): {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(s):\n",
    "    return re.sub(r'\\s+', ' ', s.strip()).lower()\n",
    "\n",
    "def gold_sets(entry):\n",
    "    by_label = {}\n",
    "    for ent in entry['entities']:\n",
    "        by_label.setdefault(ent['label'], set()).add(normalize_text(ent['text']))\n",
    "    return by_label\n",
    "\n",
    "def pred_sets_from_bert(text):\n",
    "    ents = extract_entities_bert(text)\n",
    "    by_label = {}\n",
    "    for e in ents:\n",
    "        by_label.setdefault(e['label'], set()).add(normalize_text(e['text']))\n",
    "    return by_label\n",
    "\n",
    "def parse_chat_json(raw):\n",
    "    # Attempt to parse JSON object with keys PERSON/ORG/LOC\n",
    "    try:\n",
    "        obj = json.loads(raw)\n",
    "        return {k: set(normalize_text(x) for x in v) for k, v in obj.items() if isinstance(v, list)}\n",
    "    except Exception:\n",
    "        # Fallback: extract content between first '{' and last '}'\n",
    "        m = re.search(r'\\{.*\\}', raw, re.S)\n",
    "        if m:\n",
    "            try:\n",
    "                obj = json.loads(m.group(0))\n",
    "                return {k: set(normalize_text(x) for x in v) for k, v in obj.items() if isinstance(v, list)}\n",
    "            except Exception:\n",
    "                pass\n",
    "        return {}\n",
    "\n",
    "def pred_sets_from_chat(text, few_shot=False):\n",
    "    raw = generate_json_entities(text, few_shot=few_shot)\n",
    "    return parse_chat_json(raw)\n",
    "\n",
    "def prf(gold, pred, labels=('PERSON','ORG','LOC')):\n",
    "    metrics = {}\n",
    "    tp=fp=fn=0\n",
    "    for lbl in labels:\n",
    "        g = gold.get(lbl, set())\n",
    "        p = pred.get(lbl, set())\n",
    "        t = len(g & p)\n",
    "        f_p = len(p - g)\n",
    "        f_n = len(g - p)\n",
    "        prec = t / (t + f_p) if (t + f_p) else 0.0\n",
    "        rec = t / (t + f_n) if (t + f_n) else 0.0\n",
    "        f1 = 2*prec*rec/(prec+rec) if (prec+rec) else 0.0\n",
    "        metrics[lbl] = {'precision': prec, 'recall': rec, 'f1': f1}\n",
    "        tp += t; fp += f_p; fn += f_n\n",
    "    micro_prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    micro_rec = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    micro_f1 = 2*micro_prec*micro_rec/(micro_prec+micro_rec) if (micro_prec+micro_rec) else 0.0\n",
    "    return metrics, {'precision': micro_prec, 'recall': micro_rec, 'f1': micro_f1}\n",
    "\n",
    "# Evaluate both methods\n",
    "results = {'bert': [], 'chat_zero': [], 'chat_three': []}\n",
    "for entry in DATA:\n",
    "    gold = gold_sets(entry)\n",
    "    pred_b = pred_sets_from_bert(entry['text'])\n",
    "    pred_z = pred_sets_from_chat(entry['text'], few_shot=False)\n",
    "    pred_3 = pred_sets_from_chat(entry['text'], few_shot=True)\n",
    "    m_b, micro_b = prf(gold, pred_b)\n",
    "    m_z, micro_z = prf(gold, pred_z)\n",
    "    m_3, micro_3 = prf(gold, pred_3)\n",
    "    results['bert'].append(micro_b)\n",
    "    results['chat_zero'].append(micro_z)\n",
    "    results['chat_three'].append(micro_3)\n",
    "\n",
    "def summarize(ms):\n",
    "    arr_p = [x['precision'] for x in ms]\n",
    "    arr_r = [x['recall'] for x in ms]\n",
    "    arr_f = [x['f1'] for x in ms]\n",
    "    return {'precision': float(np.mean(arr_p)), 'recall': float(np.mean(arr_r)), 'f1': float(np.mean(arr_f))}\n",
    "\n",
    "print('BERT micro (avg):', summarize(results['bert']))\n",
    "print('Chat zero-shot micro (avg):', summarize(results['chat_zero']))\n",
    "print('Chat three-shot micro (avg):', summarize(results['chat_three']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721107a0",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Extend the dataset and experiments:\n",
    "- Add entity types (e.g., `DATE`, `EVENT`) and 10 more sentences to `lab3/data/ner_examples.json`.\n",
    "- Compare BERT vs Gemma in zero-shot vs three-shot settings.\n",
    "- Improve chat prompts to reduce false positives and duplicates.\n",
    "- Report micro and per-label F1, and discuss error cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_instr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
